#llm_service
import re
import requests
from openai import OpenAI
from google import genai
from google.genai import types
from google.genai.types import GenerateContentConfig, Content
from collections import defaultdict
from telegram.helpers import escape_markdown

from config import (
    GEMINI_API_KEY, OPEN_ROUTER_API_KEY, WORKER_URL,
    SYSTEM_PROMPT_INSPECTOR, SYSTEM_PROMPT_CHAT, DEFAULT_OPENROUTER_MODELS, GEMINI_MODELS, API_TIMEOUT
)
from web_utils import get_web_context
from utils import safe_format_to_html, get_model_short_name

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ ---
or_client = OpenAI(
    api_key=OPEN_ROUTER_API_KEY,
    base_url=f"{WORKER_URL}/v1",
    timeout=API_TIMEOUT
)

gemini_client = genai.Client(
    api_key=GEMINI_API_KEY,
    http_options=types.HttpOptions(base_url=WORKER_URL, timeout=int(API_TIMEOUT * 1000)),
)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π ---
current_free_or_models = [DEFAULT_OPENROUTER_MODELS]
GEMINI_MODEL_BY_ID = {str(i): m for i, m in enumerate(GEMINI_MODELS)}
OPENROUTER_MODEL_BY_ID = {}
chat_histories = defaultdict(list)
BLACKLISTED_MODELS = set()


def fetch_dynamic_models():
    url = "https://openrouter.ai/api/v1/models"
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            all_models = response.json().get('data', [])

            # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ä—É—á–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            result = [m for m in DEFAULT_OPENROUTER_MODELS]

            for m in all_models:
                m_id = m.get('id', '')
                pricing = m.get('pricing', {})
                is_free = pricing.get('prompt') == "0" and pricing.get('completion') == "0"

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ä—É—á–Ω–æ–º —Å–ø–∏—Å–∫–µ
                if is_free and m_id not in result and m_id != "openrouter/free":
                    if m_id not in BLACKLISTED_MODELS:
                        result.append(m_id)

            return result[:15]
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
    return DEFAULT_OPENROUTER_MODELS
# llm_service.py

current_free_or_models = []  # –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏


def update_model_mappings():
    global current_free_or_models, OPENROUTER_MODEL_BY_ID

    new_models = fetch_dynamic_models()  # –¢–≤–æ—è –ª–æ–≥–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞

    if new_models:
        current_free_or_models.clear()  # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–π —Å–ø–∏—Å–æ–∫
        current_free_or_models.extend(new_models)  # –ù–∞–ø–æ–ª–Ω—è–µ–º —Ç–æ—Ç –∂–µ —Å–∞–º—ã–π –æ–±—ä–µ–∫—Ç

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ ID
        OPENROUTER_MODEL_BY_ID.clear()
        for i, m in enumerate(current_free_or_models):
            OPENROUTER_MODEL_BY_ID[str(i + 100)] = m
# llm_service.py

# –î–æ–±–∞–≤–ª—è–µ–º mode="chat" –≤ –∞—Ä–≥—É–º–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏
async def process_llm(update, context, query, selected_model=None, selected_provider=None, thread_id=None, mode="chat"):
    chat_id = update.effective_chat.id
    system_instruction = SYSTEM_PROMPT_INSPECTOR if mode == "inspector" else SYSTEM_PROMPT_CHAT
    status_msg = await context.bot.send_message(chat_id, "‚ö° –†–∞–±–æ—Ç–∞—é...", message_thread_id=thread_id)

    # –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –¥–ª—è –∏–Ω—Å–ø–µ–∫—Ç–æ—Ä–∞
    final_query = query
    if mode == "inspector":
        search_term = query.replace("–û–ë–™–ï–ö–¢ –ü–†–û–í–ï–†–ö–ò:", "").split("\n\n–í–û–ü–†–û–°:")[0].strip()
        web_data = await get_web_context(search_term)
        if web_data:
            final_query = f"–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –°–ï–¢–ò:\n{web_data}\n\n–ó–ê–î–ê–ß–ê: –ü—Ä–æ–≤–µ–¥–∏ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥: {search_term}"

    # –ò—Å—Ç–æ—Ä–∏—è
    history = chat_histories[chat_id]
    history.append(Content(role="user", parts=[types.Part(text=final_query)]))
    chat_histories[chat_id] = history[-6:]

    # –û–ß–ï–†–ï–î–¨ –ú–û–î–ï–õ–ï–ô: –°–Ω–∞—á–∞–ª–∞ Trinity (–∏–ª–∏ —Ç–æ —á—Ç–æ –≤—ã–±—Ä–∞–ª —é–∑–µ—Ä), –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
    models_to_try = []
    if selected_model and selected_provider:
        models_to_try.append((selected_provider, selected_model))

    # –î–æ–±–∞–≤–ª—è–µ–º Gemini –≤ –∫–æ–Ω–µ—Ü –∫–∞–∫ –Ω–∞–¥–µ–∂–Ω—ã–π –±—ç–∫–∞–ø
    for m in GEMINI_MODELS:
        if ("gemini", m) not in models_to_try:
            models_to_try.append(("gemini", m))

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ OpenRouter
    for m in current_free_or_models:
        if ("openrouter", m) not in models_to_try:
            models_to_try.append(("openrouter", m))



    reply_text, used_model, used_prov = None, None, None

    for prov, m_path in models_to_try:
        if m_path in BLACKLISTED_MODELS:
            continue

        try:
            name_short = get_model_short_name(m_path, prov)
            await context.bot.edit_message_text(f"üîÑ –ü—Ä–æ–±—É—é {prov}: {name_short}...", chat_id, status_msg.message_id)

            if prov == "gemini":
                # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ config.py GEMINI_MODELS –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ "models/"
                resp = gemini_client.models.generate_content(
                    model=m_path,
                    contents=[Content(role="model", parts=[types.Part(text=system_instruction)])] + history
                )
                reply_text = resp.text
            else:
                messages = [{"role": "system", "content": system_instruction}]
                for h in history:
                    messages.append({"role": "user" if h.role == "user" else "assistant", "content": h.parts[0].text})

                resp = or_client.chat.completions.create(
                    model=m_path,
                    messages=messages,
                    temperature=0.7,  # –£–º–µ–Ω—å—à–∞–µ–º "—Ñ–∞–Ω—Ç–∞–∑–∏–∏", –¥–µ–ª–∞–µ–º –æ—Ç–≤–µ—Ç—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ–µ
                    top_p=0.9,  # –û—Ç—Å–µ–∫–∞–µ–º —Å–æ–≤—Å–µ–º –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
                    extra_body={
                        "repetition_penalty": 1.1  # –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ —Ñ—Ä–∞–∑—ã
                    }
                )
                reply_text = resp.choices[0].message.content

            if reply_text:
                used_model, used_prov = name_short, prov.capitalize()
                break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ {m_path}: {e}")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –±–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–∞ 401/403/404 (–º–æ–¥–µ–ª–∏ –Ω–µ—Ç –∏–ª–∏ –ª–∏–º–∏—Ç)
            if "404" in str(e) or "401" in str(e):
                BLACKLISTED_MODELS.add(m_path)

    # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if reply_text:
        # 1. –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–∏–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏)
        # –≠—Ç–æ –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã —Å–∏–º–≤–æ–ª—ã —Ç–∏–ø–∞ '-' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –ª–æ–º–∞–ª–∏ MarkdownV2
        header_text = escape_markdown(f"{used_prov}: {used_model}", version=2)

        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç (*) –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ —Å—Ç–∏–ª–µ MarkdownV2
        formatted = f"*{header_text}*\n\n{safe_format_to_html(reply_text)}"

        try:
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=status_msg.message_id,
                text=formatted,
                parse_mode="MarkdownV2"  # –ú–ï–ù–Ø–ï–ú –ù–ê MarkdownV2
            )
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ MarkdownV2: {e}")
            # –ï—Å–ª–∏ Telegram –≤—Å–µ —Ä–∞–≤–Ω–æ —Ä—É–≥–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—Ç–∫—É, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
            clean_text = escape_markdown(reply_text, version=2)
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=status_msg.message_id,
                text=f"*{header_text}*\n\n{clean_text}",
                parse_mode="MarkdownV2"
            )

        chat_histories[chat_id].append(Content(role="model", parts=[types.Part(text=reply_text)]))
    else:
        await context.bot.edit_message_text(
            chat_id=chat_id,
            message_id=status_msg.message_id,
            text=r"‚ùå *–í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã\. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ\.*",
            parse_mode="MarkdownV2"
        )